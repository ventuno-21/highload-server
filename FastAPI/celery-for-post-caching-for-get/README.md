### [Part 1](#intro) - Introduction
### [Part 2](#install) - How to run
### [Part 3](#test) - How to test
### [Part 3-1](#ex) - Full example of testing
### [Part 4](#production) - Best way to run in Production
### [Part 4-1](#41) - Why Gunicorn ?
### [Part 4-2](#42) - Why Caching ?
### [Part 4-3](#43) - Why Celery ?
---
### PART 1 -Introduction  <a id="intro"></a>
---

Simple FastAPI + Async SQLAlchemy example (Dockerized)

This project is a Dockerized FastAPI application designed to handle high-concurrency requests efficiently. It uses SQLAlchemy 2 (async) for database interactions and includes two main endpoints: a GET endpoint to fetch items and a POST endpoint to create new items. The application is production-ready, running with Gunicorn and UvicornWorker, which allows multiple worker processes and threads to handle hundreds of simultaneous requests while maintaining fast response times.

Additionally, the project supports automatic database migrations using Alembic, making it easy to keep the database schema up-to-date. Load testing can be performed using tools like wrk for GET-heavy endpoints and hey for POST-heavy endpoints, ensuring that the service performs reliably under real-world traffic. This setup makes the project scalable, stable, and suitable for deployment in production environments.

```
â”œâ”€ app/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ main.py              # FastAPI app + routes
â”‚  â”œâ”€ models.py            # SQLAlchemy models
â”‚  â”œâ”€ schemas.py           # Pydantic schemas
â”‚  â”œâ”€ db.py                # async DB session
â”‚  â”œâ”€ crud.py              # async DB operations
â”‚  â”œâ”€ tasks.py             # Celery tasks
â”œâ”€ alembic/                # Ø¨Ø±Ø§ÛŒ migration
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â””â”€ alembic.ini
```
---
### Part 2 - How to run <a id="install"></a>
---
After pulling or Cloning the repo, follow below steps:


1. Build & start:


```bash
docker-compose up --build -d

```
2. Do migrations:
If you colonize this step is done, but in a case to add new models or change them you have to proceed this step with your specific comment.
  
first 

```
docker-compose run --rm api alembic revision --autogenerate -m "create items table"
```
then 
```
docker-compose run --rm api alembic upgrade head


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
```
psql -U postgres -d db
```

```
SELECT * FROM alembic_version;
```
```
DELETE FROM alembic_version WHERE version_num='56fd7c30fb14';
```
```
4. Delete the containers if you want to update sth:

```bash
 docker-compose down -v      
```



---
### Part 3 - How to test  <a id="test"></a>
---
Install hey or wrk.

Test GET with 100 concurrent for 30s:
```
wrk -t10 -c100 -d30s http://localhost:8000/items
```
Test POST 100 concurrent 1000 requests:
```
hey -n 1000 -c 100 -m POST -H "Content-Type: application/json" -d '{"name":"x","description":"y"}' http://localhost:8000/items
```

#### 1. What are wrk and hey?

Both are load testing / stress testing tools. They simulate multiple simultaneous requests to your API and measure:

Throughput (requests per second)

Latency (response times)

Success/failure rate

This helps you see how well your service handles high concurrency.

#### 2. wrk

Great for GET / read-heavy endpoints.

Can generate hundreds or thousands of requests per second.

Options:

-t10 â†’ number of threads sending requests (CPU-bound)  
-c100 â†’ number of concurrent connections  
-d30s â†’ duration of the test  

Example:
```
wrk -t10 -c100 -d30s http://localhost:8000/items
```

Meaning:
10 threads  
100 concurrent connections  
30 seconds of sending GET requests to /items  

ğŸ’¡ Output: shows throughput, latency, and number of successful/failed requests.


#### 3. hey

Simple and lightweight, perfect for POST / write-heavy endpoints with JSON data.

Options:

-n 1000 â†’ total number of requests  
-c 100 â†’ number of concurrent requests  
-m POST â†’ HTTP method  
-H "Content-Type: application/json" â†’ header  
-d '{"name":"x","description":"y"}' â†’ JSON payload  

Example:
```
hey -n 1000 -c 100 -m POST -H "Content-Type: application/json" -d '{"name":"x","description":"y"}' http://localhost:8000/items
```

Meaning:
1000 POST requests  
100 concurrent requests  
Sending JSON {"name":"x","description":"y"}  
To the /items endpoint  

ğŸ’¡ Output: shows latency, success rate, errors, and RPS (requests per second).

#### 4. Why are they not in requirements.txt?

wrk and hey are independent programs, not Python packages.

You install them on the OS, not with pip.

Installation examples:

#### wrk:
```
sudo apt install wrk       # Ubuntu/Debian
brew install wrk           # macOS
```
#### hey:
go install github.com/rakyll/hey@latest  
They are used only for performance testing after building and running your API.  
They do not need to be in Python requirements.

ğŸ’¡ Tip:

Use wrk for GET-heavy endpoints  
Use hey for POST-heavy / JSON-heavy endpoints  
Make sure DB and cache are ready before running load tests for realistic results.  

---
### Part 3-1 - Full example of testing  <a id="ex"></a>
---
Full example of testing
a full example of load testing your Dockerized FastAPI project with wrk and hey, including what kind of output you would see.

#### 1. Setup

Assuming your project is running in Docker Compose:
```
docker-compose up -d
```

Your FastAPI app should be accessible at:
```
http://localhost:8000
```

Endpoints:

GET /items

POST /items with JSON { "name": "x", "description": "y" }

#### 2. Test GET requests with wrk

Command:
```
wrk -t10 -c100 -d30s http://localhost:8000/items
```

Explanation:

-t10 â†’ 10 threads sending requests  
-c100 â†’ 100 concurrent connections  
-d30s â†’ run the test for 30 seconds  

Example Output (simulated):
```
Running 30s test @ http://localhost:8000/items
  10 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.5ms    1.2ms  30.0ms   95%
    Req/Sec    4200.0    300.0   4600.0    90%
  126,000 requests in 30.10s, 12.5MB read
Requests/sec:  4189.5
Transfer/sec:    425.6KB
```

Interpretation:

Latency: most requests complete in 2â€“3 ms  
Req/sec: ~4,100 requests per second  
126,000 requests completed in 30 seconds  

Shows the throughput and performance of your GET endpoint under load

#### 3. Test POST requests with hey

Command:
```
hey -n 1000 -c 100 -m POST \
    -H "Content-Type: application/json" \
    -d '{"name":"x","description":"y"}' \
    http://localhost:8000/items
```

Explanation:

-n 1000 â†’ total 1000 POST requests  
-c 100 â†’ 100 concurrent requests  

Sends JSON payload to /items

Example Output (simulated):
```
Summary:
  Total:        5.1230 secs
  Slowest:      0.0450 secs
  Fastest:      0.0010 secs
  Average:      0.0120 secs
  Requests/sec: 195.0
  Total data:   48.0 KB
Response time histogram:
  0.001 [50] |
  0.005 [500] |â– â– â– â– â– â– â– â– â– â– â– â– â– â– 
  0.010 [300] |â– â– â– â– â– â– â– â– â– â– 
  0.020 [100] |â– â– â– 
  0.045 [50]  |
Status code distribution:
  [200] 1000 responses
```

Interpretation:

Average response time: ~12 ms  
Requests/sec: ~195 (POST is heavier than GET)  
All 1000 requests succeeded with status 200  
Histogram shows most requests are fast, some slightly slower  

#### 4. Notes

For realistic testing, make sure:  

Database pool (pool_size) is sufficient  
Gunicorn workers and threads are tuned (--workers 4 --threads 2)  
Async endpoints are used properly  
GET endpoints are generally faster than POST because POST writes to DB.  
You can adjust concurrency in wrk or hey to simulate heavier or lighter load.  
If you want, I can create a cheat sheet for load testing your Dockerized FastAPI app with:  
Recommended wrk / hey commands  
Expected throughput for GET and POST  
Tuning parameters for Gunicorn + DB pool  
This will make it easy to reproduce and monitor performance.  

---
### Part 4 - Best way to run in Production level <a id="production"></a>
---
âœ… Final Architecture Summary (English)
ğŸ”¹ POST /items

The API returns immediately.

The data is pushed to RabbitMQ.

A worker receives the job â†’ processes it asynchronously using SQLAlchemy async â†’ stores it in the database.

After saving, it invalidates any cached GET results.

ğŸ”¹ GET /items

If the cache exists â†’ response is returned in milliseconds.

If not â†’ data is fetched from the database and stored in Redis for future requests.

ğŸ”¹ Gunicorn + UvicornWorker

Fully async event loop.

Easily handles ~1000 concurrent connections without blocking.

ğŸ”¹ Full Dockerized Setup

Each service runs independently.

Everything can be scaled horizontally.

Ideal for load testing and production deployment.

---
### Part 4-1 - Why Gunicorn ? <a id="41"></a>
---

Why use Gunicorn + UvicornWorker in Production?   

FastAPI itself runs on an ASGI server like Uvicorn or Hypercorn. But:   
- Uvicorn alone runs only a single process.
- If the CPU or thread is busy, other requests have to wait.
- Gunicorn can spawn multiple worker processes.
- Each worker is independent and can handle requests in parallel.

UvicornWorker tells Gunicorn:  
â€œI am async, so I can handle I/O-bound requests (DB, network) concurrently.â€  

Advantages:  
- High concurrency: hundreds of requests can be processed simultaneously.
- Stability: if one worker crashes, the others continue running.
- CPU utilization: multiple workers make full use of CPU cores.
- Async + Sync combination: threads for concurrent requests, async for I/O-heavy tasks.

####  Do we always need this setup?

Short answer: Not always, but for **production** and **high load**, it is **almost** always recommended.

When itâ€™s not needed:
Testing or development environment with few users.  
Very lightweight apps or simple APIs with no heavy DB/I/O.  
When you just want to run Uvicorn directly for local testing.  

When it is recommended:  
Production with many concurrent users or I/O-heavy requests.  
Projects with DB, Redis, file I/O, or external network calls.  
When you want your app to be crash-proof and scalable.  

#### Summary

**Development**: Uvicorn alone is enough.

**Production**: Gunicorn + UvicornWorker is better, especially if:

1. You have multiple CPU cores  
2. You expect many concurrent requests
3. Your app is async-heavy 

ğŸ’¡ Tip: For very high load, you can also add Nginx + Gunicorn + multiple containers later for extra scalability.


####  Command details
Full command:
```bash
gunicorn app.main:app -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000 --workers 4 --threads 2
```
1. Gunicorn  
Gunicorn is a WSGI/ASGI server that runs Python apps in production.

Its main job is managing multiple workers and distributing incoming requests among them.

Reason for using: Uvicorn alone is fine for development or low load, but for production, Gunicorn + workers is better.

2. app.main:app

This tells Gunicorn which FastAPI app to run:

app â†’ folder name of your project  
main â†’ the file main.py  
app â†’ the FastAPI instance (app = FastAPI(...))  

Without this, Gunicorn wouldnâ€™t know which app to start.

3.  -k uvicorn.workers.UvicornWorker

-k specifies the worker class.

UvicornWorker is an async worker, which can handle multiple concurrent I/O-bound requests (like DB or network calls).  
Without this, Gunicorn defaults to synchronous workers, which cannot properly handle async FastAPI endpoints.  

4.  --bind 0.0.0.0:8000

This makes Gunicorn listen on all network interfaces (0.0.0.0) and port 8000.  
Important so that the container can accept requests from outside or via Docker Compose.  

5.  --workers 4

Number of independent processes Gunicorn runs.  
Each worker can handle multiple requests.  
Rule of thumb: workers â‰ˆ number of CPU cores.  

Example: 4 CPU cores â†’ 4 workers.  

6.  --threads 2

Each worker can spawn multiple threads.  
More threads allow higher concurrency.  
Example: 4 workers Ã— 2 threads â†’ higher capacity for concurrent requests.  

ğŸ”¹ Summary

This command ensures:

FastAPI runs safely and reliably in production.  
Multiple processes + threads increase concurrency.  
Async I/O (DB, Redis, RabbitMQ) is properly handled.  
In Docker Compose, it can handle many simultaneous requests efficiently.  

ğŸ’¡ Tip:

You can also add --worker-connections â†’ maximum simultaneous connections per async worker.  

For high load (like 1000 concurrent requests), tuning workers, threads, and DB pool_size is important for optimal throughput.


### How It Works

```

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        Gunicorn Master       â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                     â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Worker 1     â”‚                     â”‚  Worker 2     â”‚
   â”‚ (Process)     â”‚                     â”‚ (Process)     â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Thread 1      â”‚                      â”‚ Thread 1      â”‚
 â”‚ Async handlingâ”‚                      â”‚ Async handlingâ”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ Thread 2      â”‚                      â”‚ Thread 2      â”‚
 â”‚ Async handlingâ”‚                      â”‚ Async handlingâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

... continues for Worker 3, Worker 4 if --workers=4
```

Gunicorn Master Process  
Controls all workers.  

Distributes incoming HTTP requests to the workers.  

Monitors them, restarts crashed workers automatically.  

#### Workers (Processes)
- Each worker is a separate Python process.
- Multiple workers allow CPU-bound tasks to run in parallel.
- In our setup, --workers=4 means 4 separate processes.
- Threads in Each Worker
- Each worker can spawn multiple threads (--threads=2).
- Threads help with more concurrent requests, especially small CPU-bound tasks.
- Async Handling Inside Worker
- Each thread can handle multiple async I/O tasks (DB queries, - network calls) concurrently.
- This is done by UvicornWorker, which allows async FastAPI endpoints to be non-blocking.

So even if 1 thread is waiting for a DB response, other requests are processed.

ğŸ”¹ Total Concurrency Estimate

Letâ€™s say:

--workers=4  
--threads=2  

Each async worker can handle dozens of async connections

Then total concurrency is roughly:
```
workers Ã— threads Ã— async connections per thread
= 4 Ã— 2 Ã— ~50
â‰ˆ 400â€“500 simultaneous requests
```

Actual number depends on DB pool size, network latency, and CPU usage.

ğŸ”¹ Key Takeaways

Single Uvicorn â†’ only 1 process â†’ limited by CPU â†’ can block requests.

Gunicorn + UvicornWorker â†’ multiple processes + async â†’ high concurrency.

Threads + async â†’ more requests handled concurrently per worker.

Ideal for production with many simultaneous users and I/O-heavy endpoints.


---
### Part 4-2 - Why Caching ? <a id="42"></a>
---
#### Why We Use Caching (and which type)?

Caching is essentially a shortcut: instead of asking the database the same question repeatedly, we keep the answer closeâ€”like keeping your frequently used tools in your pocket instead of the toolbox in the garage.

#### Why caching was needed here:  
- GET /items receives extremely high read traffic (1000+ concurrent requests).
- Reading from Postgres repeatedly under heavy load becomes expensive.
- Redis serves data from memory in a few microseconds.
- It reduces DB load dramatically and stabilizes latency during high traffic.

#### Caching Models (types of caching):
- Per-Request / In-Memory Cache (local LRU)
Works only inside the same process.
Fast but useless in multi-container deployments.
- Database Query Cache
Some ORMs or databases do this.
But itâ€™s unpredictable and not controlled by your application.
- Full-Object or Response Caching (what we used)  
Cache the final JSON response returned to the user.  
Extremely fast.  
Easy to invalidate when POST changes the data.  
Works perfectly with distributed systems and Docker. 

#### Why we chose Response Caching (Redis):
- Predictable and controllable.
- Independent of ORM or DB details.
- Simple invalidation strategy:  
write â†’ delete cache â†’ next GET repopulates it.
- Scales horizontally (multiple FastAPI containers use the same Redis).
- This model is the most reliable for high-traffic read endpoint



---
### Part 4-3 - Why Celery ? <a id="43"></a>
---

Placing POST operations inside a background worker removes the heavy lifting from the API server.
Think of it as handing a box to a conveyor belt instead of carrying it yourself while customers are queuing.

#### Reasons for using Celery / RabbitMQ:  
- POST can be slow (validation, DB inserts, extra business logic).
- The API server must stay free to answer new requests.
- Workers process jobs safely, one by one, with retries.
- If traffic spikes, you just scale workersâ€”not the API.

#### Do real production systems use Celery for CREATE operations?
- Yes â€” but only when the write operation is heavy or the write volume is extremely high.

#### Celery is used for POST operations when:
- The created object triggers heavy side effects
(e.g., notifications, PDFs, video processing, ML models).
- The database writes require batching.
- The system must absorb traffic spikes gracefully.
- Thereâ€™s a queue-based architecture (event-driven systems).

#### When Celery is NOT used for CREATE:
- If the database write is light and fast.
- If the endpoint returns created data immediately.
- If you don't want eventual consistency (slight delay before data appears).

#### Where Celery is commonly used in real FastAPI high-load systems:
- Sending emails/SMS/notifications
- Processing images/video/audio
- Writing logs or analytics
- Billing workflows
- Background data imports
-Running ML tasks
- Heavy CRUD operations under massive traffic
- Anything that must survive retries or avoid blocking the API

#### Using Celery for a CREATE endpoint is absolutely real-worldâ€”especially when:
- You expect thousands of concurrent POST requests.
- Your API must remain fast and never block.
- You want to fully separate API workloads from heavy workloads.

